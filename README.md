# super-repos-mlai

## Offline RL 
### 2021   
* Confident Off-Policy Evaluation and Selection through Self-Normalized Importance Weighting (AISTATS'21)  
Kuzborskij , Ilja; Vernade, Claire; Gyorgy, Andras; Szepesvari, Csaba
* Uniform Convergence in Offline Policy Evaluation and Learning for Reinforcement Learning (AISTATS'21)  
Yin, Ming; Bai, Yu; Wang, Yu-Xiang  
* Off-policy Evaluation in Infinite-Horizon Reinforcement Learning with Latent Confounders (AISTATS'21)  
Bennett, Andrew; Kallus, Nathan; Li, Lihong; Mousavi, Ali  
* Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning (AISTATS'21)  
Zhou, Zhengqing; Bai, Qinxun; Zhou, Zhengyuan; Qiu, Linhai; Blanchet, Jose; Glynn, Peter

## Online RL 
### 2021 
* Low-Rank Generalized Linear Bandit Problems (AISTATS'21)  
Lu, Yangyi; Meisami, Amirhossein; Tewari, Ambuj  
* Self-Concordant Analysis of Generalized Linear Bandits with Forgetting (AISTATS'21)  
Russac, Yoan; Faury, Louis; Capp√©, Olivier; Garivier, Aur√©lien  
* Sample Complexity Bounds for Two Timescale Value-based Reinforcement Learning Algorithms (AISTATS'21)  
Xu, Tengyu; Liang, Yingbin  
* Tractable contextual bandits beyond realizability (AISTATS'21)  
Krishnamurthy, Sanath Kumar; Hadad, Vitor; Athey, Susan  
* ùëÑ -learning with Logarithmic Regret (AISTATS'21)  
Yang, Kunhe; Yang, Lin; Du, Simon  
* An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling (AISTATS'21)  
DING, QIN; Hsieh, Cho-Jui; Sharpnack, James 
* Reinforcement Learning in Parametric MDPs with Exponential Families (AISTATS'21)  
Ray Chowdhury, Sayak; Gopalan, Aditya; Maillard, Odalric  
* Approximately Solving Mean Field Games via Entropy-Regularized Deep Reinforcement Learning (AISTATS'21)  
Cui, Kai; Koeppl, Heinz  
* On Linear Convergence of Policy Gradient Methods for Finite MDPs (AISTATS'21)
Bhandari, Jalaj; Russo, Daniel  
* Logistic Q-Learning (AISTATS'21)  
Bas Serrano, Joan; Curi, Sebastian; Krause, Andreas; Neu, Gergely

## DL theory 
### 2021  
* Regularization Matters: A Nonparametric Perspective on Overparametrized Neural Network (AISTATS'21)  
Hu, Tianyang; Wang, Wenjia; Lin, Cong; Cheng, Guang  
* A Theoretical Analysis of Catastrophic Forgetting through the NTK Overlap Matrix (AISTATS'21)  
Doan, Thang; Abbana Bennani, Mehdi; Mazoure, Bogdan; Rabusseau, Guillaume; Alquier, Pierre  
* Deep Neural Networks Are Congestion Games: From Loss Landscape to Wardrop Equilibrium and Beyond (AISTATS'21)  
Vesseron, Nina; Redko, Ievgen; Laclau, Charlotte 
* Towards a Theoretical Understanding of the Robustness of Variational Autoencoders (AISTATS'21)  
Camuto, Alexander; Willetts, Matthew; Roberts, Stephen; Holmes, Chris; Rainforth, Tom

## Computational Information Geometry  
### 2021  
* Measure Transport with Kernel Stein Discrepancy (AISTATS'21)  
Fisher, Matthew A; Nolan, Tui; Graham, Matthew; Prangle, Dennis; Oates, Chris  
* Optimal Quantisation of Probability Measures Using Maximum Mean Discrepancy (AISTATS'21)  
Teymur, Onur; Gorham, Jackson C; Riabiz, Marina; Oates, Chris  
* On Riemannian Stochastic Approximation Schemes with Fixed Step-Size (AISTATS'21)  
Durmus, Alain; Jim√©nez, Pablo; Moulines, Eric; SAID, Salem  
* Momentum Improves Optimization on Riemannian Manifolds (AISTATS'21)  
Alimisis, Foivos; Orvieto, Antonio; Becigneul, Gary; Lucchi, Aurelien  
* Shadow Manifold Hamiltonian Monte Carlo (AISTATS'21)  
van der Heide, Chris; Roosta, Fred; Hodgkinson, Liam; Kroese, Dirk  
* On the Convergence of Gradient Descent in GANs: MMD GAN As a Gradient Flow (AISTATS'21)  
Mroueh, Youssef; Nguyen, Truyen V. 
* Improved Complexity Bounds in Wasserstein Barycenter Problem (AISTATS'21)  
Dvinskikh, Darina; Tiapkin, Daniil  
* Entropy Partial Transport with Tree Metrics: Theory and Practice (AISTATS'21)  
Le, Tam; Nguyen, Truyen

## Information Geometric Perspective of Generalization in Deep Neural Networks 
* A geometric interpretation of stochastic gradient descent using diffusion metrics (Entropy'20)   
Fioresi Rita, Chaudhari Pratik, S. Saito 

## Generalization of overparameterized models   
Classicial statistical learning theory bounds the generalization by capacity-based measure such as VC dimension and Rademacher complexity. These methods often rely on standard ideas of uniform convergence over a model class with small capacity-based measure (e.g., small VC dimension). However, in modern machine learning, overparameterized model class render such capacity-based bounds vacuous while surprisingly possesses a good generalization. In particular, a overparameterized model can overfit a training data (zero training error) but still generalizes well to the unseen data. This phenomenon urges a new understanding of generalization beyond uniform convergence for overparameterized model and overfitting problem. 

* Begnin overfitting in linear regression (NAS'21)  
Bartlett et al.  
* Does data interpolation contradict statistical optimality? (AISTATS'19)   
Belkin et al. 
